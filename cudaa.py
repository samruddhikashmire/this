# -*- coding: utf-8 -*-
"""cudaa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8VbzoIFoTy5YpPxvrqY2znXevzvMBhC
"""

## practical 4

# Commented out IPython magic to ensure Python compatibility.
# %%writefile multiply.cu
# #include <stdio.h>
# #include <cuda_runtime.h>
# 
# __global__ void matMul(float *A , float *B , float *C , int N) {
#     int row = blockIdx.y * blockDim.y + threadIdx.y;
#     int col = blockIdx.x * blockDim.x + threadIdx.x;
# 
#     if (row < N && col < N) {
#         float sum = 0;
#         for (int i = 0; i < N; i++) {
#             sum += A[row * N + i] * B[i * N + col];
#         }
#         C[row * N + col] = sum;
#     }
# }
# 
# int main() {
#     int N = 2;
#     size_t size = N * N * sizeof(float);
# 
#     float A[] = {1, 2, 3, 4};
#     float B[] = {5, 6, 7, 8};
#     float C[4];
# 
#     float *a, *b, *c;
#     cudaMalloc(&a, size);
#     cudaMalloc(&b, size);
#     cudaMalloc(&c, size);
# 
#     cudaMemcpy(a, A, size, cudaMemcpyHostToDevice);
#     cudaMemcpy(b, B, size, cudaMemcpyHostToDevice);
# 
#     dim3 threads(16, 16);
#     dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);
# 
#     matMul<<<blocks, threads>>>(a, b, c, N);
# 
#     // Add this to wait for GPU to finish before copying
#     cudaDeviceSynchronize();
# 
# 
#     cudaMemcpy(C, c, size, cudaMemcpyDeviceToHost);
# 
#     printf("Result Matrix C:\n");
#     for (int row = 0; row < N; row++) {
#         for (int col = 0; col < N; col++) {
#             printf("%f ", C[row * N + col]);
#         }
#         printf("\n");
#     }
# 
# 
#     cudaFree(a);
#     cudaFree(b);
#     cudaFree(c);
#     return 0;
# }
#

!nvcc -arch=sm_70 multiply.cu -o multiply

!./multiply

# Commented out IPython magic to ensure Python compatibility.
# %%writefile addition.cu
# #include <stdio.h>
# 
# __global__ void vecAdd(float *A , float *B , float *C , int N)
# {
#     int i = blockIdx.x * blockDim.x + threadIdx.x;
#     if(i < N)
#     {
#         C[i] = A[i] + B[i];
#     }
# }
# 
# int main()
# {
#     int N = 5 ;
#     size_t size = N *sizeof(float);
#     float A[] = {1,2,3,4,5};
#     float B[] = {6,7,8,9,10};
#     float C[5];
# 
#     float *a , *b , *c;
#     cudaMalloc(&a , size);
#     cudaMalloc(&b , size);
#     cudaMalloc(&c , size);
# 
#     cudaMemcpy(a , A , size , cudaMemcpyHostToDevice);
#     cudaMemcpy(b , B , size , cudaMemcpyHostToDevice);
# 
#     vecAdd<<<1 , N>>>(a , b , c , N);
# 
#     cudaMemcpy(C , c , size , cudaMemcpyDeviceToHost);
# 
#     printf("Result Vector C:\n");
#     for(int i=0;i<N;i++)
#     {
#         printf("%f " , C[i]);
#     }
#     printf("\n");
# 
#     cudaFree(c);
#     cudaFree(b);
#     cudaFree(a);
#     return 0;
# 
# 
# }

!nvcc -arch=sm_70 addition.cu -o addition

!./addition

